{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn operadora Machine Learning (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "\n",
    "import re\n",
    "\n",
    "# IQR/Z score.\n",
    "from scipy.stats import stats\n",
    "\n",
    "# Seleción de variables.\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Modelos Selección.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Modelos.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# Preprocesado.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "# Métricas.\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, auc, confusion_matrix, f1_score, precision_score, recall_score, roc_curve, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar los datasets y construir un único tablón analítico con todas las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_clientes = pd.read_csv(\"clientes_diciembre.csv\", sep = \"|\", parse_dates = [\"antiguedad\"])\n",
    "dic_consumos = pd.read_csv(\"consumos_diciembre.csv\", sep = \"|\")\n",
    "dic_fin = pd.read_csv(\"financiacion_diciembre.csv\", sep = \"|\")\n",
    "dic_prod = pd.read_csv(\"productos_diciembre.csv\", sep = \"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de tabla de consumos diciembre: 95467\n",
      "Tamaño de tabla de clientes diciembre: 95467\n",
      "Tamaño de tabla de productos diciembre: 95467\n",
      "Tamaño de tabla de financiación diciembre: 24198\n"
     ]
    }
   ],
   "source": [
    "print('Tamaño de tabla de consumos diciembre:',dic_consumos.shape[0])\n",
    "print('Tamaño de tabla de clientes diciembre:',dic_clientes.shape[0])\n",
    "print('Tamaño de tabla de productos diciembre:',dic_prod.shape[0])\n",
    "print('Tamaño de tabla de financiación diciembre:',dic_fin.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clientes en tabla consumos\n",
      " True    95467\n",
      "Name: id, dtype: int64\n",
      "Clientes en tabla productos\n",
      " True    95467\n",
      "Name: id, dtype: int64\n",
      "Clientes en tabla financiacion\n",
      " False    71269\n",
      "True     24198\n",
      "Name: id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Clientes en tabla consumos\\n',dic_clientes['id'].isin(dic_consumos['id']).value_counts())\n",
    "print('Clientes en tabla productos\\n',dic_clientes['id'].isin(dic_prod['id']).value_counts())\n",
    "print('Clientes en tabla financiacion\\n', dic_clientes['id'].isin(dic_fin['id']).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_cl_con = pd.merge(dic_clientes, dic_consumos, on = \"id\", how = \"left\")\n",
    "dic_cl_con_f = pd.merge(dic_cl_con, dic_fin, on= \"id\", how = \"left\")\n",
    "dic_full = pd.merge(dic_cl_con_f, dic_prod, on = \"id\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>edad</th>\n",
       "      <th>facturacion</th>\n",
       "      <th>antiguedad</th>\n",
       "      <th>provincia</th>\n",
       "      <th>num_lineas</th>\n",
       "      <th>num_dt</th>\n",
       "      <th>incidencia</th>\n",
       "      <th>num_llamad_ent</th>\n",
       "      <th>num_llamad_sal</th>\n",
       "      <th>mb_datos</th>\n",
       "      <th>seg_llamad_ent</th>\n",
       "      <th>seg_llamad_sal</th>\n",
       "      <th>financiacion</th>\n",
       "      <th>imp_financ</th>\n",
       "      <th>descuentos</th>\n",
       "      <th>conexion</th>\n",
       "      <th>vel_conexion</th>\n",
       "      <th>TV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>216.028109</td>\n",
       "      <td>2018-11-23 08:48:00</td>\n",
       "      <td>La Rioja</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110</td>\n",
       "      <td>79</td>\n",
       "      <td>10897</td>\n",
       "      <td>12806</td>\n",
       "      <td>13751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FIBRA</td>\n",
       "      <td>50MB</td>\n",
       "      <td>tv-futbol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>84</td>\n",
       "      <td>255.830842</td>\n",
       "      <td>2017-08-22 03:19:00</td>\n",
       "      <td>Vizcaya</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189</td>\n",
       "      <td>89</td>\n",
       "      <td>18657</td>\n",
       "      <td>6499</td>\n",
       "      <td>10862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SI</td>\n",
       "      <td>FIBRA</td>\n",
       "      <td>600MB</td>\n",
       "      <td>tv-futbol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>135.768153</td>\n",
       "      <td>2001-12-27 13:50:00</td>\n",
       "      <td>Albacete</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129</td>\n",
       "      <td>30</td>\n",
       "      <td>15511</td>\n",
       "      <td>17013</td>\n",
       "      <td>16743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SI</td>\n",
       "      <td>ADSL</td>\n",
       "      <td>35MB</td>\n",
       "      <td>tv-futbol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>69</td>\n",
       "      <td>255.658527</td>\n",
       "      <td>2015-08-08 10:53:00</td>\n",
       "      <td>Lugo</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>52</td>\n",
       "      <td>12670</td>\n",
       "      <td>3393</td>\n",
       "      <td>6771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FIBRA</td>\n",
       "      <td>200MB</td>\n",
       "      <td>tv-familiar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>22.302845</td>\n",
       "      <td>1997-08-29 02:19:00</td>\n",
       "      <td>Tarragona</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>183</td>\n",
       "      <td>3</td>\n",
       "      <td>23756</td>\n",
       "      <td>18436</td>\n",
       "      <td>4485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ADSL</td>\n",
       "      <td>10MB</td>\n",
       "      <td>tv-futbol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  edad  facturacion          antiguedad  provincia  num_lineas  num_dt  \\\n",
       "0   1    63   216.028109 2018-11-23 08:48:00   La Rioja           5     NaN   \n",
       "1   2    84   255.830842 2017-08-22 03:19:00    Vizcaya           3     NaN   \n",
       "2   3    66   135.768153 2001-12-27 13:50:00   Albacete           4     NaN   \n",
       "3   4    69   255.658527 2015-08-08 10:53:00       Lugo           4     NaN   \n",
       "4   5    30    22.302845 1997-08-29 02:19:00  Tarragona           2     2.0   \n",
       "\n",
       "  incidencia  num_llamad_ent  num_llamad_sal  mb_datos  seg_llamad_ent  \\\n",
       "0        NaN             110              79     10897           12806   \n",
       "1        NaN             189              89     18657            6499   \n",
       "2        NaN             129              30     15511           17013   \n",
       "3        NaN              51              52     12670            3393   \n",
       "4        NaN             183               3     23756           18436   \n",
       "\n",
       "   seg_llamad_sal financiacion  imp_financ descuentos conexion vel_conexion  \\\n",
       "0           13751          NaN         NaN        NaN    FIBRA         50MB   \n",
       "1           10862          NaN         NaN         SI    FIBRA        600MB   \n",
       "2           16743          NaN         NaN         SI     ADSL         35MB   \n",
       "3            6771          NaN         NaN        NaN    FIBRA        200MB   \n",
       "4            4485          NaN         NaN        NaN     ADSL         10MB   \n",
       "\n",
       "            TV  \n",
       "0    tv-futbol  \n",
       "1    tv-futbol  \n",
       "2    tv-futbol  \n",
       "3  tv-familiar  \n",
       "4    tv-futbol  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95467, 19)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_clientes = pd.read_csv(\"clientes_enero.csv\", sep = \"|\", parse_dates = [\"antiguedad\"])\n",
    "en_consumos = pd.read_csv(\"consumos_enero.csv\", sep = \"|\")\n",
    "en_fin = pd.read_csv(\"financiacion_enero.csv\", sep = \"|\")\n",
    "en_prod = pd.read_csv(\"productos_enero.csv\", sep = \"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de tabla de consumos enero: 92711\n",
      "Tamaño de tabla de clientes enero: 92711\n",
      "Tamaño de tabla de productos enero: 92711\n",
      "Tamaño de tabla de financiación enero: 25332\n"
     ]
    }
   ],
   "source": [
    "print('Tamaño de tabla de consumos enero:',en_consumos.shape[0])\n",
    "print('Tamaño de tabla de clientes enero:',en_clientes.shape[0])\n",
    "print('Tamaño de tabla de productos enero:',en_prod.shape[0])\n",
    "print('Tamaño de tabla de financiación enero:',en_fin.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clientes en tabla consumos\n",
      " True    92711\n",
      "Name: id, dtype: int64\n",
      "Clientes en tabla productos\n",
      " True    92711\n",
      "Name: id, dtype: int64\n",
      "Clientes en tabla financiacion\n",
      " False    67379\n",
      "True     25332\n",
      "Name: id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Clientes en tabla consumos\\n',en_clientes['id'].isin(en_consumos['id']).value_counts())\n",
    "print('Clientes en tabla productos\\n',en_clientes['id'].isin(en_prod['id']).value_counts())\n",
    "print('Clientes en tabla financiacion\\n', en_clientes['id'].isin(en_fin['id']).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_cl_con = pd.merge(en_clientes, en_consumos, on = \"id\", how = \"left\")\n",
    "en_cl_con_f = pd.merge(en_cl_con, en_fin, on = \"id\", how = \"left\")\n",
    "en_full = pd.merge(en_cl_con_f, en_prod, on = \"id\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enero:  92711\n",
      "Diciembre:  95467\n"
     ]
    }
   ],
   "source": [
    "## Valores únicos\n",
    "print('Enero: ',len(en_full['id'].unique()))\n",
    "print('Diciembre: ',len(dic_full['id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_full[\"mes\"] = \"dic\"\n",
    "en_full[\"mes\"] = \"en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - Se observa que en enero hubo una disminución de 3000 clientes aproximadamente. Por tanto, la empresa perdió más clientes de los que ganó. \n",
    "        - __7085 clientes abandonaron la empresa.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7085"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dic_full) - dic_full.id.isin(en_full.id).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - Creación de la variable target. Se buscan los \"id\" de enero que coinciden con los de diciembre, asignando 0 si son idénticos o 1 si no lo son."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = pd.DataFrame()\n",
    "churn['churn'] = np.where(dic_full.id.isin(en_full.id) == True, 0, 1)\n",
    "churn['id'] =dic_full['id'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  Antes de concatenar las cosechas, voy a transformar la variable antigüedad\n",
    "dic_full['duracion'] = (datetime(2019,12,31) - dic_full[\"antiguedad\"])\n",
    "en_full['duracion'] = (datetime(2020,1,31) - en_full[\"antiguedad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenamos las dos cosechas y normalizamos todas las variables\n",
    "\n",
    "data = pd.concat([dic_full, en_full], sort = False, ignore_index=True)\n",
    "data.columns= data.columns.str.lower()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación de la variable antigüedad a días, horas... Esta parte podría hacerse en la parte de FE\n",
    "data[\"duracion\"] = data.duracion.apply(lambda x: int(x.days))\n",
    "\n",
    "# Month:\n",
    "data[\"month\"] = data.antiguedad.apply(lambda x: x.strftime(\"%B\"))\n",
    "\n",
    "# Lo mismo con día de la semana. \n",
    "data[\"dia\"] = data.antiguedad.apply(lambda x: x.strftime(\"%A\"))\n",
    "\n",
    "# Recoger la info de la hora.\n",
    "data[\"hora\"] = data.antiguedad.apply(lambda x: int(x.hour))\n",
    "\n",
    "# Categórica para poder tener interpretabilidad.\n",
    "data[\"hora_bin\"] = pd.cut(data[\"hora\"], bins = [-1,6,12,18,23], labels = [\"Madrugada\", \"Mañana\", \"Tarde\", \"Noche\"])\n",
    "\n",
    "# Se elimina la variable original pues ya está convertida a numérica.\n",
    "data.drop(columns = \"antiguedad\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre procesado y  limpieza de los datos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols= data.select_dtypes(include=['object','category']).columns\n",
    "num_cols = data.select_dtypes(exclude=['object','category']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cat_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[num_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tratamiento de nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Las variables incidencia, financiación, descuentos cuando están a nulo significa que es NO (0)\n",
    "\n",
    "var_no = ['incidencia','financiacion','descuentos']\n",
    "\n",
    "for i in var_no:\n",
    "    data[i].fillna('NO',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Las variables num_dt, imp_financ cuando están a nulo significa que es el valor es 0.\n",
    "var_cero = ['num_dt','imp_financ']\n",
    "\n",
    "for i in var_cero:\n",
    "    data[i].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['vel_conexion'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['vel_conexion']=='?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['vel_conexion']=='1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['vel_conexion']=='16598']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adsl = data.query('conexion == \"ADSL\"')\n",
    "adsl.groupby('vel_conexion')['conexion'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos que dentro de las conexiones adsl, la velocidad de conexión más repetida es la de 20 MB \n",
    "data['vel_conexion'] = data['vel_conexion'].replace('1', '20MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fibra = data.query('conexion == \"FIBRA\"')\n",
    "fibra.groupby('vel_conexion')['conexion'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos que dentro de las conexiones fibra, la velocidad de conexión más repetida es la de 200 MB \n",
    "data['vel_conexion'] = data['vel_conexion'].replace('?', '200MB')\n",
    "data['vel_conexion'] = data['vel_conexion'].replace('16598', '200MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Faltan por rellenar unos pocos nulos en la variable conexión y en la variable vel_conexion\n",
    "\n",
    "print(data.conexion.unique())\n",
    "\n",
    "print(data.conexion.value_counts())\n",
    "\n",
    "## La probabilidad de rellenar el valor con ADSL o FIBRA es prácticamente el mismo. En este caso, podríamos rellenarlo con la moda ( ADSL ) o incluso eliminar las dos líneas\n",
    "\n",
    "data['conexion'].fillna('ADSL',inplace=True)\n",
    "\n",
    "print(data.vel_conexion.unique())\n",
    "\n",
    "print(data.vel_conexion.value_counts())\n",
    "\n",
    "## En el caso de la velocidad, rellenaremos con el valor 200MB (moda). En este caso, se podría haber rellenado también de varias maneras. Normalmente en estos casos, tenemos alguna información más de negocio (velocidad más probable...)\n",
    "\n",
    "data['vel_conexion'].fillna('200MB',inplace=True)\n",
    "\n",
    "## Vemos también 3 valores que trataremos después"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables numéricas\n",
    "fig, axs = plt.subplots(ncols=5, nrows=3, figsize=(20, 10))\n",
    "axs = axs.flatten() # \n",
    "\n",
    "index = 0\n",
    "for k,v in data[num_cols].items(): \n",
    "    if  (k == 'permanencia'):\n",
    "        sns.countplot(v, ax=axs[index])\n",
    "    else:\n",
    "        sns.distplot(v, bins=20, ax=axs[index])\n",
    "    index += 1\n",
    "\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables categóricas\n",
    "fig, axs = plt.subplots(ncols=5, nrows=3, figsize=(20, 10))\n",
    "axs = axs.flatten() # \n",
    "\n",
    "index = 0\n",
    "for k,v in data[cat_cols].items(): \n",
    "    sns.countplot(v, ax=axs[index])\n",
    "    index += 1\n",
    "\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación variables a numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_dummies = ['conexion','tv']\n",
    "variables_binarias = ['financiacion','descuentos','incidencia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in variables_dummies:\n",
    "    dummies = pd.get_dummies(data[k],prefix=k)\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop(k, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in variables_binarias:\n",
    "    data[j] = [1 if x == 'SI' else 0 for x in data[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"vel_conexion\"] = data.vel_conexion.apply(lambda x: int(re.sub(\"MB\", \"\", x)))\n",
    "data.vel_conexion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables nuevas\n",
    "\n",
    "data.month.replace(('January','February','March','April','May','June','July','August','September','October','November','December'),\n",
    "                      (1,2,3,4,5,6,7,8,9,10,11,12),inplace=True)\n",
    "\n",
    "data.dia.replace(('Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'),\n",
    "                      (1,2,3,4,5,6,7),inplace=True)\n",
    "\n",
    "data.hora_bin.replace(('Madrugada','Mañana','Tarde','Noche'),\n",
    "                      (1,2,3,4),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtenemos la comunidad autónoma de cada provincia\n",
    "url = \"https://es.wikipedia.org/wiki/Provincia_(Espa%C3%B1a)\"\n",
    "html = requests.get(url).content\n",
    "df_list = pd.read_html(html)\n",
    "ccaa = df_list[1]\n",
    "ccaa = ccaa[[\"Comunidad autónoma\", \"Provincia\"]]\n",
    "ccaa.columns =ccaa.columns.str.lower()\n",
    "ccaa = ccaa.rename(columns = {'comunidad autónoma': 'com_aut'})\n",
    "ccaa.provincia.replace(('Baleares'),('Islas Baleares'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data,ccaa,how='left',on='provincia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('provincia', axis=1, inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.com_aut.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.com_aut.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.com_aut.replace(('La Rioja', 'País Vasco', 'Castilla-La Mancha', 'Galicia',\n",
    "                       'Cataluña', 'Andalucía', 'Comunidad Valenciana',\n",
    "                       'Comunidad de Madrid', 'Islas Canarias', 'Castilla y León', 'Aragón',\n",
    "                       'Navarra', 'Extremadura', 'Asturias',\n",
    "                       'Islas Baleares', 'Cantabria', 'Región de Murcia'),\n",
    "                        (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.com_aut = data.com_aut.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_correlacion = ['edad', 'facturacion', 'num_lineas', 'num_dt', 'incidencia',\n",
    "       'num_llamad_ent', 'num_llamad_sal', 'mb_datos', 'seg_llamad_ent',\n",
    "       'seg_llamad_sal', 'financiacion', 'imp_financ', 'descuentos',\n",
    "       'vel_conexion','duracion', 'month', 'dia', 'hora',\n",
    "       'hora_bin','com_aut']\n",
    "\n",
    "df_correlacion = data[list_correlacion]\n",
    "correlation_mat = df_correlacion.corr()\n",
    "sns.heatmap(correlation_mat)\n",
    "plt.show()\n",
    "\n",
    "## Vemos correlación entre las variables num_lineas y facturacion y seg_llamd_sali y financiación, aunque muy baja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_correlacion:\n",
    "    sns.boxplot(x=data[i])\n",
    "    print(i)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "## Vemos posibles outliers en vel conexión y num_lineas. Vamos a comprobarlo con z_score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "z = np.abs(stats.zscore(data['vel_conexion']))\n",
    "print(z)\n",
    "print(np.where(z > 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "z = np.abs(stats.zscore(data['num_lineas']))\n",
    "print(z)\n",
    "print(np.where(z > 3))\n",
    "\n",
    "## Hay 3 valores que están fuera del rango. Podemos eliminar esas filas\n",
    "\n",
    "data.drop([2580,8657,36987], axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Todos los valores cumplen con el estandar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dic = pd.merge(data[data['mes']=='dic'], churn, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dic.drop('mes', axis=1, inplace=True)\n",
    "df_dic.drop('hora_bin', axis=1, inplace=True)\n",
    "df_dic.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Las variables más importantes del modelo son num_dt, incidencia, descuentos, financiacion, importe financiacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr = abs(df_dic.corr())\n",
    "corr[['churn']].sort_values(by = 'churn',ascending = False).style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proporciones_final (var,target,df):\n",
    "\n",
    "    proporcion = pd.DataFrame()\n",
    "    \n",
    "    proporcion['%fugas'] = df[target].groupby(df[var]).mean()*100\n",
    "    proporcion['Conteo'] = df[target].groupby(df[var]).count()\n",
    "    proporcion= proporcion.round(3)   \n",
    "    proporcion_filtered = proporcion[(proporcion['%fugas']>0) & (proporcion['Conteo']>10)]  \n",
    "        \n",
    "    if len(proporcion_filtered)<100 and len(proporcion_filtered)>1:\n",
    "        fig = plt.figure()\n",
    "        ax = proporcion_filtered['Conteo'].plot(kind='bar',grid=True)\n",
    "                \n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(proporcion_filtered['%fugas'].values, linestyle='-', linewidth=2.0,color='g')\n",
    "        plt.ylim(0, 100) # modificación.\n",
    "        plt.tight_layout()   \n",
    "        \n",
    "    else:        \n",
    "        proporcion_filtered.reset_index(inplace=True)\n",
    "        sns.lmplot(x = var,y ='%fugas', data=proporcion_filtered,fit_reg=True,ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in df_dic.columns:\n",
    "    proporciones_final (i, 'churn', df_dic)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "df_dic['edad_joven'] = df_dic['edad'].apply(lambda x: 1 if x<=24 else 0)\n",
    "df_dic['num_dt_bin'] = df_dic['num_dt'].apply(lambda x: 1 if x>=1 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Muestrear los datos, construyendo un dataset de train y test\n",
    "\n",
    "   - Muestreo estratificado en función de la variable respuesta para que en los splits sus categorías estén bien representadas.\n",
    "       - Split 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split (df_dic.drop('churn', axis=1), \n",
    "                                                     df_dic.churn,\n",
    "                                                     test_size = 0.2,\n",
    "                                                     random_state = 0,\n",
    "                                                     stratify = df_dic.churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construir un modelo analítico de clasificación que sea capaz de predecir cuando un cliente se fuga de la empresa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saca_metricas(y_real, y_pred): \n",
    "    false_positive_rate, recall, thresholds = roc_curve(y_real, y_pred)\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "    print('- AUC: {roc_auc}')\n",
    "    plt.plot(false_positive_rate, recall, 'b')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.title('AUC = %0.2f' % roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"ejercicio_churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Modelo y predicciones:\n",
    "tree_mod = DecisionTreeClassifier(criterion=\"gini\").fit(X_train, y_train)\n",
    "tree_pred = tree_mod.predict(X_test)\n",
    "\n",
    "# Métricas:\n",
    "saca_metricas(y_test, tree_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='decission_tree'):\n",
    "\n",
    "    # predict_proba returns [prob_negative, prob_positive], so slice the output with [:, 1]\n",
    "    accuracy = accuracy_score(y_test, tree_pred)\n",
    "    false_positive_rate, recall, thresholds = roc_curve(y_test, tree_pred)\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "   \n",
    "    # Use the area under the ROC curve as a metric.\n",
    "    mlflow.log_metric('accuracy', accuracy)\n",
    "    mlflow.log_metric('roc_auc', roc_auc)\n",
    "    # log model\n",
    "    mlflow.sklearn.log_model(tree_mod,'tree_ntic')\n",
    "    # save model\n",
    "    mlflow.sklearn.save_model(tree_mod, 'tree.pkl',\n",
    "                              serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = RandomForestClassifier().fit(X_train, y_train)\n",
    "pred2 = classifier2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saca_metricas(y_test, pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='random_forest'):\n",
    "\n",
    "    # predict_proba returns [prob_negative, prob_positive], so slice the output with [:, 1]\n",
    "    accuracy = accuracy_score(y_test, pred2)\n",
    "    false_positive_rate, recall, thresholds = roc_curve(y_test, pred2)\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "   \n",
    "    # Use the area under the ROC curve as a metric.\n",
    "    mlflow.log_metric('accuracy', accuracy)\n",
    "    mlflow.log_metric('roc_auc', roc_auc)\n",
    "    # log model\n",
    "    mlflow.sklearn.log_model(classifier2,'rf_ntic')\n",
    "    # save model\n",
    "    mlflow.sklearn.save_model(tree_mod, 'rf.pkl',\n",
    "                              serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier3 = LinearSVC(penalty='l1', dual= False).fit(X_train, y_train)\n",
    "pred3 = classifier3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saca_metricas(y_test, pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier4 = LogisticRegression(max_iter=10000).fit(X_train, y_train)\n",
    "pred4 = classifier4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saca_metricas(y_test, pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier5 = GaussianNB().fit(X_train, y_train)\n",
    "pred5 = classifier5.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saca_metricas(y_test, pred5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='gaussiano'):\n",
    "\n",
    "    # predict_proba returns [prob_negative, prob_positive], so slice the output with [:, 1]\n",
    "    accuracy = accuracy_score(y_test, pred5)\n",
    "    false_positive_rate, recall, thresholds = roc_curve(y_test, pred5)\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "   \n",
    "    # Use the area under the ROC curve as a metric.\n",
    "    mlflow.log_metric('accuracy', accuracy)\n",
    "    mlflow.log_metric('roc_auc', roc_auc)\n",
    "    # log model\n",
    "    mlflow.sklearn.log_model(classifier5,'gaus_ntic')\n",
    "    # save model\n",
    "    mlflow.sklearn.save_model(tree_mod, 'gaus.pkl',\n",
    "                              serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación cruzada\n",
    "\n",
    "cv = cross_val_score(\n",
    "    classifier2,\n",
    "    X_train, \n",
    "    y_train,\n",
    "    scoring = \"roc_auc\",\n",
    "    cv = 5\n",
    ")\n",
    "print(cv)\n",
    "print(\"CV ROC:\", cv.mean(), np.std(cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = {}\n",
    "for i in range(len(X_train.columns)):\n",
    "    imp[X_train.columns[i]] = [classifier2.feature_importances_[i]]\n",
    "pd.DataFrame.from_dict(imp, orient=\"index\", columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False).head(20).style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podemos comparar varios modelos/hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](recursive.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_rcv = {\n",
    "    \"XGB\": XGBClassifier(n_jobs=-1, n_estimators=30, random_state=1234),\n",
    "    \"RF\": RandomForestClassifier(n_estimators =30, n_jobs=-1, random_state=1234),\n",
    "    \"Tree\": DecisionTreeClassifier(random_state=1234),\n",
    "    \"Log\": LogisticRegression(solver=\"newton-cg\", penalty=\"l2\"),\n",
    "}\n",
    "\n",
    "def rskf_comparison(models, X_train, y_train):\n",
    "    \n",
    "    results = []\n",
    "    names = []\n",
    "\n",
    "    for k, v in models.items():\n",
    "    \n",
    "        rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1234) \n",
    "        cv_scores = cross_val_score(v, X_train, y_train, scoring='roc_auc', cv=rskf, n_jobs=-1)\n",
    "\n",
    "        results.append(cv_scores)\n",
    "        names.append(k)\n",
    "\n",
    "        print(k)\n",
    "        print('CV AUC: %.5f +/- %.5f' % (np.mean(cv_scores), np.std(cv_scores)))\n",
    "        print('-------------------------')\n",
    "        \n",
    "    \n",
    "    return(results, names)\n",
    "\n",
    "\n",
    "results, names = rskf_comparison(models_rcv, X_train, y_train)\n",
    "plt.figure(figsize=(10, 6))\n",
    "comparison = plt.boxplot(results)\n",
    "plt.xticks(np.arange(1,len(names)+1),names)\n",
    "plt.show(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tuning hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_param = {\n",
    "    'n_estimators': [100,300,500],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [16,20,22,24],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_kfold = StratifiedKFold(n_splits=5,\n",
    "                                       shuffle=True,\n",
    "                                       random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid = GridSearchCV(estimator=classifier2,\n",
    "                     param_grid=grid_param,\n",
    "                     scoring='roc_auc',\n",
    "                     cv=stratified_kfold,\n",
    "                     n_jobs=-1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(model_grid.best_params_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(model_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = RandomForestClassifier(criterion = 'entropy', n_estimators= 500, max_depth = 16, max_features= 'auto').fit(X_train, y_train)\n",
    "pred2 = classifier2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se podría mejorar el modelo balanceando datos, trameando variables, creando variables nuevas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(df_dic.churn==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {}\".format(sum(df_dic.churn==0)))\n",
    "\n",
    "sm = SMOTE(sampling_strategy=0.4, random_state=2)\n",
    "df_dic_res, y_res = sm.fit_resample(df_dic.drop('churn', axis=1), df_dic.churn.ravel())\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(df_dic_res.shape))\n",
    "print('After OverSampling, the shape of train_y: {}'.format(y_res.shape))\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_res==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_res==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res, X_test_res, y_train_res, y_test_res = train_test_split (df_dic_res, \n",
    "                                                     y_res,\n",
    "                                                     test_size = 0.2,\n",
    "                                                     random_state = 0,\n",
    "                                                     stratify = y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier5 = RandomForestClassifier(criterion = 'entropy', n_estimators= 500, max_depth = 16, max_features= 'auto').fit(X_train_res, y_train_res)\n",
    "pred5 = classifier5.predict(X_test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_res,pred5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saca_metricas(y_test_res,pred5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier5.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = classifier5.feature_importances_.argsort()\n",
    "plt.barh(X_train_res.columns, classifier5.feature_importances_[sorted_idx])\n",
    "plt.xlabel(\"Random Forest Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(classifier5.feature_importances_, index=X_train_res.columns)\n",
    "feat_importances.sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filename = \"modelo1.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(classifier5, file)\n",
    "\n",
    "# Load from file\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jt -t monokai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
